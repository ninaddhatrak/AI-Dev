{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76c3e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba3adc",
   "metadata": {},
   "source": [
    "## 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e62f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8799 posts\n"
     ]
    }
   ],
   "source": [
    "# Load JSONL data\n",
    "data = []\n",
    "with open('data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        if 'data' in entry:\n",
    "            data.append(entry['data'])\n",
    "        else:\n",
    "            data.append(entry)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Loaded {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986cbb4",
   "metadata": {},
   "source": [
    "## 2. Prepare Text for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5a75c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts:\n",
      "\n",
      "1. What Are You Reading/Book Club Tuesday.  What you are reading, watching, or listening to? Or how far have you gotten in your chosen selection since last week?...\n",
      "\n",
      "2. \"WTF is Social Ecology?\" by Usufruct Collective....\n",
      "\n",
      "3. Who do you think is the most powerful/popular anarch-nihilist ever?. I am an anarcho-nihilist and i am reading similar type of books for a long time. Many of the authors i read, had really close conne...\n"
     ]
    }
   ],
   "source": [
    "# Combine title and selftext into a single text field\n",
    "def prepare_text(row):\n",
    "    title = str(row.get('title', '')) if row.get('title') else ''\n",
    "    selftext = str(row.get('selftext', '')) if row.get('selftext') else ''\n",
    "    # Combine title and selftext\n",
    "    combined = f\"{title}. {selftext}\".strip()\n",
    "    # Limit length to avoid memory issues (model has 256 token limit anyway)\n",
    "    return combined[:2000] if combined else title\n",
    "\n",
    "df['text_for_embedding'] = df.apply(prepare_text, axis=1)\n",
    "\n",
    "# Preview\n",
    "print(\"Sample texts:\")\n",
    "for i, text in enumerate(df['text_for_embedding'].head(3)):\n",
    "    print(f\"\\n{i+1}. {text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50fd9b",
   "metadata": {},
   "source": [
    "## 3. Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b0a3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen3-Embedding-0.6B model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f7de1d1e934a9eb3d09ded811dbbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Load Qwen3-Embedding-0.6B model\n",
    "# ~600MB, 1024 dimensions, high quality multilingual embeddings\n",
    "print(\"Loading Qwen3-Embedding-0.6B model...\")\n",
    "model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B', trust_remote_code=True)\n",
    "print(f\"Model loaded! Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26a8e0",
   "metadata": {},
   "source": [
    "## 4. Compute Embeddings (Parallelized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f44240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 8799 posts...\n",
      "Batch size: 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4bb9c761484b0aad1295accd457fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings computed! Shape: (8799, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings in batches (faster than one-by-one)\n",
    "# The model already uses GPU if available and handles batching internally\n",
    "\n",
    "texts = df['text_for_embedding'].tolist()\n",
    "batch_size = 32  # Process in batches for efficiency\n",
    "\n",
    "print(f\"Computing embeddings for {len(texts)} posts...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Use model's built-in parallel processing\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,  # L2 normalize for cosine similarity\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\nEmbeddings computed! Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d839d7d",
   "metadata": {},
   "source": [
    "## 5. Append Embeddings to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b4d87bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'embedding' column to dataframe\n",
      "Embedding dimension: 1024\n",
      "\n",
      "Sample embedding (first 10 values):\n",
      "[-0.0159912109375, -0.040771484375, -0.003997802734375, -0.09130859375, 0.059814453125, -0.0283203125, 0.05126953125, 0.014404296875, -0.031494140625, -0.0260009765625]\n"
     ]
    }
   ],
   "source": [
    "# Add embeddings as a new column (as list for JSON serialization)\n",
    "df['embedding'] = [emb.tolist() for emb in embeddings]\n",
    "\n",
    "# Verify\n",
    "print(f\"Added 'embedding' column to dataframe\")\n",
    "print(f\"Embedding dimension: {len(df['embedding'].iloc[0])}\")\n",
    "print(f\"\\nSample embedding (first 10 values):\")\n",
    "print(df['embedding'].iloc[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40d1f1",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4397d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving: 100%|██████████| 8799/8799 [00:02<00:00, 2949.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 8799 posts with embeddings to 'data_with_embeddings.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save to new JSONL file with embeddings\n",
    "output_file = 'data_with_embeddings.jsonl'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Saving\"):\n",
    "        # Convert row to dict and write as JSON line\n",
    "        record = row.to_dict()\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"\\nSaved {len(df)} posts with embeddings to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bffb6",
   "metadata": {},
   "source": [
    "## 7. Quick Validation - Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf2ec4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First post:\n",
      "  Title: What Are You Reading/Book Club Tuesday...\n",
      "\n",
      "Most similar posts:\n",
      "\n",
      "1. (similarity: 1.000)\n",
      "   Title: What Are You Reading/Book Club Tuesday...\n",
      "\n",
      "2. (similarity: 0.482)\n",
      "   Title: Friday Free Talk...\n",
      "\n",
      "3. (similarity: 0.412)\n",
      "   Title: \"WTF is Social Ecology?\" by Usufruct Collective...\n",
      "\n",
      "4. (similarity: 0.371)\n",
      "   Title: HOW TO ORGANIZE YOUR COMMUNITY...\n",
      "\n",
      "5. (similarity: 0.365)\n",
      "   Title: Reading \"1922: The Hong Kong strike\"...\n"
     ]
    }
   ],
   "source": [
    "# Quick test: Find similar posts to the first one\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarity matrix for first 100 posts\n",
    "sample_embeddings = np.array(df['embedding'].head(100).tolist())\n",
    "similarity_matrix = cosine_similarity(sample_embeddings)\n",
    "\n",
    "# Find most similar posts to the first one\n",
    "first_post_similarities = similarity_matrix[0]\n",
    "most_similar_indices = np.argsort(first_post_similarities)[::-1][1:6]  # Top 5 (excluding itself)\n",
    "\n",
    "print(\"First post:\")\n",
    "print(f\"  Title: {df.iloc[0]['title'][:80]}...\")\n",
    "print(f\"\\nMost similar posts:\")\n",
    "for i, idx in enumerate(most_similar_indices, 1):\n",
    "    print(f\"\\n{i}. (similarity: {first_post_similarities[idx]:.3f})\")\n",
    "    print(f\"   Title: {df.iloc[idx]['title'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fac03821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "           EMBEDDING SUMMARY\n",
      "==================================================\n",
      "Total posts processed: 8799\n",
      "Model used: Qwen3-Embedding-0.6B\n",
      "Embedding dimension: 1024\n",
      "Output file: data_with_embeddings.jsonl\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\"*50)\n",
    "print(\"           EMBEDDING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total posts processed: {len(df)}\")\n",
    "print(f\"Model used: Qwen3-Embedding-0.6B\")\n",
    "print(f\"Embedding dimension: {len(df['embedding'].iloc[0])}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-product-developement-hZ-ZWPTx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
